{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANfBloGyVq50"
   },
   "source": [
    "# Question Answering with BERT\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<img src=\"example-report.png\" width=\"30%\" align=\"right\" style=\"padding-left:20px\">\n",
    "\n",
    "In this notebook, I've built a Question-Answering pipline that utilizes the transformer-based, pre-trained [BERT](https://github.com/google-research/bert) model to answer questions related to a given passage. At the end, I have applied this pipeline on questions linked to some medical-related passages such as clinical notes. The pipeline is made of following pieces:\n",
    "\n",
    "- [1. Prepare the input](#1): Preprocessing text for input to the BERT model\n",
    "- [2. Find candidate answers](#2): Given the passage and the question, the BERT model outputs candidate answers in terms of the score (i.e. logits) of the starting and ending indices in the array of the tokenized passage.\n",
    "- [3. Choose the most likely answer](#3): From the list of scores obtained in previous step, choose the pair of (start index, end index) which maximizes their combined scores.  \n",
    "- [4. Construct the final answer](#3): The final answer is constructed by stiching all words that are located between start index and end index (obtained from previous step) in the given passage. \n",
    "\n",
    "This project is inspired by the [work](https://arxiv.org/abs/1901.07031) done by Irvin et al.\n",
    "\n",
    "<a href=\"https://ieeexplore.ieee.org/document/7780643\">Image Credit</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following libraries for this assignment.\n",
    "\n",
    "- `tensorflow` - standard deep learning library\n",
    "- `transformers` - convenient access to pretrained natural language models\n",
    "\n",
    "Additionally, load the helper `question_answer_util` module that contains all the functions associated with the Question/Answering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "\n",
    "from question_answer_util import *\n",
    "\n",
    "# watch for any changes in the text_process_util module, and reload it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ius9KDaXXZk"
   },
   "source": [
    "<a name='2-2'></a>\n",
    "### 1. Preparing the input\n",
    "\n",
    "\n",
    "We need to first prepare the raw passage and question for input into the BERT model. In this regard, we first apply a pre-built tokenizer to both passage and question, which maps each word in those pieces of text to a unique element in the vocabulary. We also insert special tokens. In other words, given the strings `p` (i.e. passage) and `q` (i.e. question), we want to turn them into an input of the following form: \n",
    "\n",
    "`[CLS]` `[q_token1]`, `[q_token2]`, ..., `[SEP]` `[p_token1]`, `[p_token2]`, ...\n",
    "\n",
    "Here, the special characters `[CLS]` and `[SEP]` let the model know which part of the input is the question and which is the answer. \n",
    "- The question appears between `[CLS]` and `[SEP]`.\n",
    "- The answer appears after `[SEP]`\n",
    "\n",
    "Next, since BERT takes in a fixed-length input, we add padding to those tokenized inputs whose length is less than a pre-specified max input length. \n",
    "\n",
    "In the test case below, prepare_bert_input(question, passage, tokenizer, max_seq_length=20),\n",
    "returns three items. \n",
    "- First is `input_ids`, which holds the numerical ids of each token. \n",
    "- Second, the `input_mask`, which has 1's in parts of the input tensor representing input tokens, and 0's where there is padding. \n",
    "- Finally, `tokens`, the output of the tokenizer (including the `[CLS]` and `[SEP]` tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Tokenizer\n",
    "\n",
    "Before using the function below, we need to  load the pre-built tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "ZaM_uXl6Z9Gl",
    "outputId": "e6a6a48b-f5fb-45b4-ef77-e9b4540b19ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case:\n",
      "\n",
      "Passage: My name is Bob.\n",
      "Question: What is my name?\n",
      "\n",
      "Tokens:\n",
      "['[CLS]', 'What', 'is', 'my', 'name', '?', '[SEP]', 'My', 'name', 'is', 'Bob', '.']\n",
      "\n",
      "Corresponding input IDs:\n",
      "tf.Tensor(\n",
      "[[ 101 1327 1110 1139 1271  136  102 1422 1271 1110 3162  119    0    0\n",
      "     0    0    0    0    0    0]], shape=(1, 20), dtype=int32)\n",
      "\n",
      "Mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "passage = \"My name is Bob.\"\n",
    "\n",
    "question = \"What is my name?\"\n",
    "\n",
    "input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, 20)\n",
    "print(\"Test Case:\\n\")\n",
    "print(\"Passage: {}\".format(passage))\n",
    "print(\"Question: {}\".format(question))\n",
    "print()\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "print(\"\\nCorresponding input IDs:\")\n",
    "print(input_ids)\n",
    "print(\"\\nMask:\")\n",
    "print(input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2. Find candidate answers\n",
    "\n",
    "The pre-trained BERT model, which is loaded below, takes in the tokenized input, returns two vectors. \n",
    "- The first vector contains the scores (more formally, logits) for starting indices of sets of possible answers. \n",
    "    - A higher score means that index is more likely to be the start of the final, chosen answer span in the passage. \n",
    "- The second vector contains the score for the ending indices of those possible answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8g_vHX3Ra0Ge"
   },
   "source": [
    "<a name='2-3'></a>\n",
    "### 3. Choose the most likely answer\n",
    "\n",
    "We use the abovementioned, two score vectors as well as the input mask to find the span of tokens from the passage that maximizes the start score and end score. \n",
    "- To be valid, the start index has to occur before the end index. Formally, we want to find:\n",
    "\n",
    "$$\\arg\\max_{i <= j, mask_i=1, mask_j = 1} start\\_scores[i] + end\\_scores[j]$$\n",
    "- In other words, this formula is saying, calculate the sum and start scores of start position 'i' and end position 'j', given the constraint that the start 'i' is either before or at the end position 'j'; then find the positions 'i' and 'j' where this sum is the highest.\n",
    "- Furthermore, we want to make sure that $i$ and $j$ are in the relevant parts of the input (i.e. where `input_mask` equals 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FUQGHgir5wwl",
    "outputId": "2c540701-7b55-47dd-db58-15d601e83c8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max start is at index i=1 and score 2.0\n",
      "max end is at index i=4 and score 4.0\n",
      "max start + max end sum of scores is 6.0\n",
      "Expected: (1, 4) \n",
      "Returned: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# test case to show how span calculations work\n",
    "\n",
    "start_scores = tf.convert_to_tensor([-1, 2, 0.4, -0.3, 0, 8, 10, 12], dtype=float)\n",
    "end_scores = tf.convert_to_tensor([5, 1, 1, 3, 4, 10, 10, 10], dtype=float)\n",
    "input_mask = [1, 1, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "start, end = get_span_from_scores(start_scores, end_scores, input_mask, verbose=True)\n",
    "\n",
    "print(\"Expected: (1, 4) \\nReturned: ({}, {})\".format(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILR_O0xh6zoJ"
   },
   "source": [
    "<a name='ex-05'></a>\n",
    "### 4. Construct the final answer\n",
    "\n",
    "Finally, we form the contiguous token from the tokenized input using the start and end indices obtained in the previous. Then, we pass the token to construct_answer(token) function, which performs some clean-up on the token and returns the final string for the answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_out_string_1: hello how  are  you?, length 20\n",
      "tmp_out_string_2: @hellohowareyou?, length 16\n"
     ]
    }
   ],
   "source": [
    "# Test case\n",
    "\n",
    "# assume this is the contiguous token that forms the final answer\n",
    "tmp_tokens_1 = [' ## hello', 'how ', 'are ', 'you?      ']\n",
    "tmp_out_string_1 = construct_answer(tmp_tokens_1)\n",
    "\n",
    "print(f\"tmp_out_string_1: {tmp_out_string_1}, length {len(tmp_out_string_1)}\")\n",
    "\n",
    "\n",
    "tmp_tokens_2 = ['@',' ## hello', 'how ', 'are ', 'you?      ']\n",
    "tmp_out_string_2 = construct_answer(tmp_tokens_2)\n",
    "print(f\"tmp_out_string_2: {tmp_out_string_2}, length {len(tmp_out_string_2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhAd2kkcbDHa"
   },
   "source": [
    "<a name=\"2-1\"></a>\n",
    "### Putting It All Together\n",
    "\n",
    "get_model_answer(\\*args, \\**kwargs) function puts all previous steps together and outputs the answer related to the input question and passage.\n",
    "\n",
    "```CPP\n",
    "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\n",
    "    \"\"\"\n",
    "    # prepare input\n",
    "    ...\n",
    "        \n",
    "    # get scores for start of answer and end of answer\n",
    "    ...\n",
    "    # using scores, get most likely answer\n",
    "    ...\n",
    "    \n",
    "    # using span start and end, construct answer as string\n",
    "    ...\n",
    "    return answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2nXup8d68y1"
   },
   "source": [
    "<a name='2-5'></a>\n",
    "### 4. Test the pipeline\n",
    "\n",
    "Now that we've prepared all the pieces, let's try an example from the SQuAD dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Computational complexity theory\n",
      "Expected: Computational complexity theory\n"
     ]
    }
   ],
   "source": [
    "passage = \"Computational complexity theory is a branch of the theory \\\n",
    "           of computation in theoretical computer science that focuses \\\n",
    "           on classifying computational problems according to their inherent \\\n",
    "           difficulty, and relating those classes to each other. A computational \\\n",
    "           problem is understood to be a task that is in principle amenable to \\\n",
    "           being solved by a computer, which is equivalent to stating that the \\\n",
    "           problem may be solved by mechanical application of mathematical steps, \\\n",
    "           such as an algorithm.\"\n",
    "\n",
    "question = \"What branch of theoretical computer science deals with broadly \\\n",
    "            classifying computational problems by difficulty and class of relationship?\"\n",
    "\n",
    "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\n",
    "print(\"Expected: Computational complexity theory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: pharma\n",
      "Expected: pharma\n"
     ]
    }
   ],
   "source": [
    "passage = \"The word pharmacy is derived from its root word pharma which was a term used since \\\n",
    "           the 15th–17th centuries. However, the original Greek roots from pharmakos imply sorcery \\\n",
    "           or even poison. In addition to pharma responsibilities, the pharma offered general medical \\\n",
    "           advice and a range of services that are now performed solely by other specialist practitioners, \\\n",
    "           such as surgery and midwifery. The pharma (as it was referred to) often operated through a \\\n",
    "           retail shop which, in addition to ingredients for medicines, sold tobacco and patent medicines. \\\n",
    "           Often the place that did this was called an apothecary and several languages have this as the \\\n",
    "           dominant term, though their practices are more akin to a modern pharmacy, in English the term \\\n",
    "           apothecary would today be seen as outdated or only approproriate if herbal remedies were on offer \\\n",
    "           to a large extent. The pharmas also used many other herbs not listed. The Greek word Pharmakeia \\\n",
    "           (Greek: φαρμακεία) derives from pharmakon (φάρμακον), meaning 'drug', 'medicine' (or 'poison').\"\n",
    "\n",
    "question = \"What word is the word pharmacy taken from?\"\n",
    "\n",
    "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))\n",
    "print(\"Expected: pharma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: therapy combined with diagnostics\n"
     ]
    }
   ],
   "source": [
    "passage = \"A City of Hope scientist and his colleagues have developed \\\n",
    "           a user-friendly approach to creating 'theranostics' \\\n",
    "           — therapy combined with diagnostics — that target specific tumors and diseases.\\\n",
    "           Key to the process are molecules called metallocorroles, \\\n",
    "           which serve as versatile platforms for the development of drugs and imaging agents. \\\n",
    "           Metallcorroles both locate (via imaging) and kill tumors. \\\n",
    "           City of Hope’s John Termini, Ph.D., and his colleagues at Caltech \\\n",
    "           and the Israel Institute of Technology have developed a novel method \\\n",
    "           to prepare cell-penetrating nanoparticles called \\\n",
    "           “metallocorrole/protein nanoparticles.” \\\n",
    "           The theranostics could both survive longer in the body \\\n",
    "           and better snipe disease targets.\"                       \n",
    "\n",
    "question = \"What does theranostics refer to?\"\n",
    "\n",
    "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it on clinical notes. Below we have an excerpt of a doctor's notes for a patient with an abnormal echocardiogram (this sample is taken from [here](https://www.mtsamples.com/site/pages/sample.asp?Type=6-Cardiovascular%20/%20Pulmonary&Sample=1597-Abnormal%20Echocardiogram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: How old is the patient?\n",
      "\n",
      "Answer: 86\n",
      "\n",
      "\n",
      "Question 2: Does the patient have any complaints?\n",
      "\n",
      "Answer: The patient complains of shortness of breath\n",
      "\n",
      "\n",
      "Question 3: What is the reason for this consultation?\n",
      "\n",
      "Answer: further evaluation\n",
      "\n",
      "\n",
      "Question 4: What does her echocardiogram show?\n",
      "\n",
      "Answer: severe mitral regurgitation and also large pleural effusion\n",
      "\n",
      "\n",
      "Question 5: What other symptoms does the patient have?\n",
      "\n",
      "Answer: colitis and also diverticulitis\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\n",
    "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\n",
    "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
    "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\n",
    "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\n",
    "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\n",
    "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
    "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\n",
    "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\n",
    "           for many years. She also was told that she has a heart murmur, which was not followed through \\\n",
    "           on a regular basis.\"\n",
    "\n",
    "q1 = \"How old is the patient?\"\n",
    "q2 = \"Does the patient have any complaints?\"\n",
    "q3 = \"What is the reason for this consultation?\"\n",
    "q4 = \"What does her echocardiogram show?\"\n",
    "q5 = \"What other symptoms does the patient have?\"\n",
    "\n",
    "\n",
    "questions = [q1, q2, q3, q4, q5]\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    print(\"Question {}: {}\".format(i+1, q))\n",
    "    print()\n",
    "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [CLS]\n"
     ]
    }
   ],
   "source": [
    "passage = \"The key to effective precision medicine is data. \\\n",
    "           A vast amount of individualized data from \\\n",
    "           hundreds of thousands of patients is required, \\\n",
    "           in order to match all possible genetic abnormalities with \\\n",
    "           their proper treatments.\"         \n",
    "           \n",
    "question = \"What does make a precision medicine procedure effective?\"\n",
    "\n",
    "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: data\n"
     ]
    }
   ],
   "source": [
    "passage = \"The key to effective precision medicine is data. \\\n",
    "           A vast amount of individualized data from \\\n",
    "           hundreds of thousands of patients is required, \\\n",
    "           in order to match all possible genetic abnormalities with \\\n",
    "           their proper treatments. \\\n",
    "           Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\n",
    "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\n",
    "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
    "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\n",
    "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\n",
    "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\n",
    "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
    "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\n",
    "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\n",
    "           for many years. She also was told that she has a heart murmur, which was not followed through \\\n",
    "           on a regular basis.\"           \n",
    "           \n",
    "question = \"What does make a precision medicine procedure effective?\"\n",
    "\n",
    "print(\"Output: {}\".format(get_model_answer(model, question, passage, tokenizer)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "C3M3_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "AI4MC3-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
